{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing what convnets learn\n",
    "\n",
    "[Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff).\n",
    "\n",
    "----\n",
    "\n",
    "It is often said that deep learning models are \"black boxes\", learning representations that are difficult to extract and present in a \n",
    "human-readable form. While this is partially true for certain types of deep learning models, it is definitely not true for convnets. The \n",
    "representations learned by convnets are highly amenable to visualization, in large part because they are _representations of visual \n",
    "concepts_.\n",
    "\n",
    "Visualizing heatmaps of class activation in an image is useful to understand which part of an image where identified as belonging to a given class, and thus allows to localize objects in images.\n",
    "\n",
    "We will use the VGG16 model to visualize heatmaps of class activation in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing heatmaps of class activation: Class Activation Map\n",
    "\n",
    "Heatmaps of class activation is useful for understanding which parts of a given image led a convnet to its \n",
    "final classification decision. This is helpful for \"debugging\" the decision process of a convnet, in particular in case of a classification \n",
    "mistake. It also allows you to locate specific objects in an image.\n",
    "\n",
    "This general category of techniques is called \"Class Activation Map\" (CAM) visualization, and consists in producing heatmaps of \"class \n",
    "activation\" over input images. A \"class activation\" heatmap is a 2D grid of scores associated with an specific output class, computed for \n",
    "every location in any input image, indicating how important each location is with respect to the class considered. For instance, given a \n",
    "image fed into one of our \"cat vs. dog\" convnet, Class Activation Map visualization allows us to generate a heatmap for the class \"cat\", \n",
    "indicating how cat-like different parts of the image are, and likewise for the class \"dog\", indicating how dog-like differents parts of the \n",
    "image are.\n",
    "\n",
    "The specific implementation we will use is the one described in [Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via \n",
    "Gradient-based Localization](https://arxiv.org/abs/1610.02391). It is very simple: it consists in taking the output feature map of a \n",
    "convolution layer given an input image, and weighing every channel in that feature map by the gradient of the class with respect to the \n",
    "channel. Intuitively, one way to understand this trick is that we are weighting a spatial map of \"how intensely the input image activates \n",
    "different channels\" by \"how important each channel is with regard to the class\", resulting in a spatial map of \"how intensely the input \n",
    "image activates the class\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "# Note that we are including the densely-connected classifier on top;\n",
    "model = VGG16(weights='imagenet')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following image of 5 Labrador retriever.\n",
    "<img src=Labrador_retriever.jpeg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this image into something the VGG16 model can read: the model was trained on images of size 224x244, preprocessed according \n",
    "to a few rules that are packaged in the utility function `keras.applications.vgg16.preprocess_input`. So we need to load the image, resize \n",
    "it to 224x224, convert it to a Numpy float32 tensor, and apply these pre-processing rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# The local path to our target image\n",
    "#img_path = './afriacan_elephant.jpg'\n",
    "#img_path = './Egyptian_cat.jpg'\n",
    "img_path = './Labrador_retriever.jpeg'\n",
    "\n",
    "# `img` is a PIL image of size 224x224\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "# `x` is a float32 Numpy array of shape (224, 224, 3)\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# We add a dimension to transform our array into a \"batch\" of size (1, 224, 224, 3)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Channel-wise color normalization\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n02099712', 'Labrador_retriever', 0.60521394), ('n02099601', 'golden_retriever', 0.34359834), ('n02104029', 'kuvasz', 0.04729474)]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The top-3 classes predicted for this image are:\n",
    "\n",
    "* Labrador retriever (with 60.52% probability)\n",
    "* Golden retriever (with 34.36% probability)\n",
    "* Kuvasz (with 4.73% probability)\n",
    "\n",
    "Thus our network has recognized this image as an undetermined quantity of Labrador retriever. The entry in the prediction vector that was maximally activated is the one corresponding to the \"Labrador retriever\" class, at index 208:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize which parts of our image were the most \"Labrador retriever\"-like, let's set up the Grad-CAM process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the \"maximally activated\" entry in the prediction vector\n",
    "max_output = model.output[:, np.argmax(preds[0])]\n",
    "\n",
    "# The is the output feature map of the `block5_conv3` layer,\n",
    "# the last convolutional layer in VGG16\n",
    "last_conv_layer = model.get_layer('block5_conv3')\n",
    "\n",
    "# This is the gradient of the predicted class with regard to\n",
    "# the output feature map of `block5_conv3`\n",
    "grads = K.gradients(max_output, last_conv_layer.output)[0]\n",
    "\n",
    "# This is a vector of shape (512,), where each entry\n",
    "# is the mean intensity of the gradient over a specific feature map channel\n",
    "pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "# This function allows us to access the values of the quantities we just defined:\n",
    "# `pooled_grads` and the output feature map of `block5_conv3`, given a sample image\n",
    "iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "\n",
    "# These are the values of these two quantities, as Numpy arrays, given our sample image\n",
    "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
    "\n",
    "# We multiply each channel in the feature map array\n",
    "# by \"how important this channel is\" with regard to the predicted class\n",
    "for i in range(512):\n",
    "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "\n",
    "# The channel-wise mean of the resulting feature map\n",
    "# is our heatmap of class activation\n",
    "heatmap = np.mean(conv_layer_output_value, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization purpose, we will also normalize the heatmap between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use OpenCV to generate an image that superimposes the original image with the heatmap we just obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# We use cv2 to load the original image\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# We resize the heatmap to have the same size as the original image\n",
    "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "# We convert the heatmap to RGB\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "# We apply the heatmap to the original image\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "# 0.4 here is a heatmap intensity factor\n",
    "superimposed_img = heatmap * 0.4 + img\n",
    "\n",
    "# Save the image to disk\n",
    "#cv2.imwrite('./African_elephant_cam.jpg', superimposed_img)\n",
    "#cv2.imwrite('./Labrador_retriever_cam.jpg', superimposed_img)\n",
    "#cv2.imwrite('./Egyptian_cat_cam.jpg', superimposed_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualisation technique answers two important questions:\n",
    "\n",
    "* Why did the network think this image contained an Labrador retriever?\n",
    "* Where is the Labrador retriever located in the picture?\n",
    "\n",
    "In particular, it is interesting to note that the ears of the puppy are strongly activated: this is probably how the network can tell the difference between Labrador retriever and Golden retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, please refer to [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
